### A Pluto.jl notebook ###
# v0.10.0

using Markdown

# ╔═╡ d54b3cd3-c52c-4db9-9267-2e8871cf4597
md"""
# Julia is Fast - `@time`, `@elapsed` and `@inbounds`

---

In this notebook, we demonstrate how fast `Julia` is, compared to other dynamically typed languages. 

## Prerequisites

Read the text [Why Julia?](https://github.com/stevengj/julia-mit/blob/master/README.md#why-julia) (3 min)

Read [Performance tips](https://docs.julialang.org/en/stable/manual/performance-tips/) section of the `Julia` manual. (20 min) 

## Competences 

The reader should understand effects of "[just-in-time
compiler](https://en.wikipedia.org/wiki/Just-in-time_compilation)"
called [LLVM](http://llvm.org/) on the speed of execution of programs. 
The reader should be able to write simple, but fast, programs containing loops.

## Credits 

Some examples are taken from [The Julia Manual](https://docs.julialang.org/en/stable/).

"""

# ╔═╡ b8c1b263-a5c5-45a2-afdc-744bdc25ca54
md"""
## Scholarly example - summing integer halves

Consider the function `f` which sums halves of integers from `1` to `n`:

__N.B.__ `Esc l` toggles line numbers in the current cell.
"""

# ╔═╡ 23d162f9-184b-415a-87a2-ccf75b6d5e9a
function f(n)
    s = 0
    for i = 1:n
        s += i/2
    end
    s
end

# ╔═╡ bb8a280d-b118-4842-9220-9382a5408ed5
md"""
In order for the fast execution, the function must first be compiled. Compilation is performed automatically, when the function is invoked for the first time. Therefore, the first call can be done with some trivial choice of parameters.

The timing can be done by two commands, `@time` and `@elapsed`: 
"""

# ╔═╡ b000b740-72e4-4056-98d9-c92608cd3a10
?@time

# ╔═╡ 28ed4062-6ad6-4a60-b772-006869bb504f
?@elapsed

# ╔═╡ e27d795b-7d7d-4e76-9ede-9e33b902c2cf
@time f(1) 

# ╔═╡ 299b40a4-4d21-46a4-b7ac-133f69a8309b
# This run is much faster, since the function is already compiled
@elapsed f(1)

# ╔═╡ 9042a5c9-97bf-4177-9585-25c4bcbd611f
md"""
Let us now run the big-size computation. Notice the unnaturally high byte allocation and the huge amount of time spent on 
[garbage collection](http://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29).
"""

# ╔═╡ a9691c4d-7f2c-461c-a42d-cfb5209f3a2d
# Notice the unnaturally high byte  allocation!
@time f(1000000)

# ╔═╡ 11d575a0-7311-4e1f-884d-f447a888602c
# We shall be using @time from now on
@elapsed f(1000000)

# ╔═╡ e5545357-9216-46f4-98d1-dfd4a3f31ff7
md"""
Since your computer can execute several _Gigaflops_ (floating-point operations per second), this is rather slow. This slowness is due to _type instability_: variable `s` is in the beginning assumed to be of type `Integer`, while at every other step, the result is a real number of type `Float64`. Permanent checking of types requires permanent memory allocation and deallocation (garbage collection). This is corrected by very simple means: just declare `s` as a real number, and the execution is more than 10 times faster with almost no memory allocation (and, consequently, no garbage collection).
"""

# ╔═╡ 6a9fd473-7939-42f6-a5e4-53dc53abe15f
function f1(n)
    s = 0.0
    for i = 1:n
        s += i/2
    end
    s
end

# ╔═╡ 93e801a3-c255-428f-878c-60c34d0d7870
@time f1(1)

# ╔═╡ 401c72ac-07e7-4ed7-9735-e2d8c846633d
@time f1(1000000)

# ╔═╡ 1ec9ad17-67b6-4778-abbd-e31b4d2b2887
md"""
`@time` can alo be invoked as a function:
"""

# ╔═╡ 557aeae6-93cb-419b-88d5-274018a95d84
@time(f1(1000000))

# ╔═╡ 8b754531-102c-4d8f-905d-afd44a354aeb
@time s2=f1(1000000)

# ╔═╡ a3ca5d12-c330-488b-8639-ad36bc98d3e7
@time(s2=f1(1000000))

# ╔═╡ 6571ae43-6583-47f9-bd6a-abe1d3a0f85a
md"""
## Real-world example - exponential moving average

[Exponential moving average](http://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) is a fast _one pass_ formula (each data point of the given data set $A$ is accessed only once) often used in high-frequency on-line trading (see [Online Algorithms in High-Frequency Trading](http://cacm.acm.org/magazines/2013/10/168184-online-algorithms-in-high-frequency-trading/fulltext) for more details). __Notice that the output array $X$ is declared in advance.__

Using `return` in the last line is here optional.

"""

# ╔═╡ 236594df-0138-4bdd-be1e-bd6a4f69fa02
function fexpma{T}( A::Vector{T}, α::T )
# fast exponential moving average: X - moving average, 
# A - data, alpha - exponential forgetting parameter
    n = length(A)
    X = Array{T}(n) # Declare X
    β = one(T)-α
    X[1] = A[1]
    for k = 2:n
        X[k] = β*A[k] + α*X[k-1]
    end
    return X
end


# ╔═╡ 692f908f-8b0b-4809-bafe-d64840650a52
# First run for compilation
fexpma([1.0],0.5)

# ╔═╡ 20fd5340-7aae-4d2b-9da9-27fe15dd1789
md"""
We now generate some big-size data:
"""

# ╔═╡ f12d97f8-498c-4a64-98ec-73486f875728
# Big random slightly increasing sequence
A=[rand() + 0.00001*k*rand() for k=1:20_000_000]

# ╔═╡ 38fad48c-2bc4-433b-80c7-b685f3076691
@time X=fexpma(A,0.9)

# ╔═╡ 94ce85a1-f6bc-483e-a3b3-70f8e4397e54
md"""
## `@inbounds`

The `@inbounds` command eliminates array bounds checking within expressions. Be certain before doing this. If the subscripts are ever out of bounds, you may suffer crashes or silent corruption. The following program runs a little faster:
"""

# ╔═╡ 02bed44e-1265-4394-87ba-e3e2c617067b
function fexpma{T}( A::Vector{T}, α::T )
# fast exponential moving average: X - moving average, 
# A - data, alpha - exponential forgetting parameter
    n = length(A)
    X = Array{T}(n) # Declare X
    β = one(T)-α
    X[1] = A[1]
    @inbounds for k = 2:n
        X[k] = β*A[k] + α*X[k-1]
    end
    return X
end

# ╔═╡ 328f3f81-4894-4b76-8d49-274fde7da898
@time X=fexpma(A,0.9)

# ╔═╡ 9d7a39c7-acc1-4d40-878d-32a367596d81
md"""
Similar `Matlab` programs give the following timing for the two versions of the function, first _without_ prior declaration of $X$ and then _with_ prior declaration. The _latter_ version is nine times faster. 

---
```
function X = fexpma0( A,alpha )
% fast exponential moving average: X - moving average, A - data, alpha - exponential forgetting parameter
n=length(A);
beta=1-alpha;
X(1)=A(1);
for k=2:n
    X(k)=beta*A(k)+alpha*X(k-1);
end
```
```
>> A=rand(20000000,1)+0.00001*[1:20000000]'.*rand(20000000,1);
>> tic, X=fexpma0(A,0.9); toc
Elapsed time is 3.073359 seconds.
```

---

```
function X = fexpma( A,alpha )
% fast exponential moving average: X - moving average, A - data, alpha - exponential forgetting parameter
n=length(A);
X=zeros(n,1); % Allocate X in advance
beta=1-alpha;
X(1)=A(1);
for k=2:n
    X(k)=beta*A(k)+alpha*X(k-1);
end
```
```
>> tic, X=fexpma(A,0.9); toc
Elapsed time is 0.320976 seconds.
```
"""

# ╔═╡ 2cf93c7e-aa9e-4c0f-8ea5-7e5985e38632
md"""
## Plotting the moving average

Let us plot the data $A$ and its exponential moving average $X$. The dimension of the data is too large for meaningful direct plot. In `Julia` we can use `@manipulate` command to slide through the data. It takes a while to read packages `Winston` (for plotting) and `Interact`, but this is needed only for the first invocation.
"""

# ╔═╡ 240e99da-c557-43b8-8ecc-f09542591f93
using Winston
using Interact

# ╔═╡ 07a499e7-6399-42e1-9677-a00d5bf95970
@manipulate for k=1:1000:20000000
    plot(collect(k:k+1000),A[k:k+1000],"r.",
        collect(k:k+1000),X[k:k+1000],"b")
end

# ╔═╡ 18f0d086-514a-4017-852e-ff172a7476fd
md"""
### Remark
More details about optimizing your programs are given in the [Profiling Notebook](http://localhost:8890/notebooks/Documents/Julia/Julia-Course/src/05%20Profiling.ipynb).
"""

# ╔═╡ 6845a906-c76e-46ec-b689-ff7cbadcbe37
md"""
## Pre-allocating output

The following example is from [Pre-allocating outputs](https://docs.julialang.org/en/stable/manual/performance-tips/#Pre-allocating-outputs-1). The functions `loopinc()` and `loopinc_prealloc()` both compute $\sum_{i=2}^{10000001}i$, the second one being 15 times faster: 
"""

# ╔═╡ 291af947-08a0-4f61-85a2-f07f7b428833
function xinc(x)
    return [x, x+1, x+2]
end

function loopinc()
    y = 0
    for i = 1:10^7
        ret = xinc(i)
        y += ret[2]
    end
    y
end

function xinc!{T}(ret::AbstractVector{T}, x::T)
    ret[1] = x
    ret[2] = x+1
    ret[3] = x+2
    nothing
end

function loopinc_prealloc()
    ret = Array{Int}(3)
    y = 0
    for i = 1:10^7
        xinc!(ret, i)
        y += ret[2]
    end
    y
end

# ╔═╡ 0fb1efd5-6a54-4963-901d-27eec5a1d0a8
@time loopinc()

# ╔═╡ cb0027b5-7afb-4d00-a5eb-a01e32cb5f7f
@time loopinc_prealloc() # After the second run

# ╔═╡ 44a8c386-f5fe-497e-b8b3-829c575d9a11
md"""
## Memory access

The following example is from [Access arrays in memory order, along columns](https://docs.julialang.org/en/stable/manual/performance-tips/#Access-arrays-in-memory-order,-along-columns-1).

Multidimensional arrays in Julia are stored in column-major order, which means that arrays are stacked one column at a time. This convention for ordering arrays is common in many languages like Fortran, Matlab, and R (to name a few). The alternative to column-major ordering is row-major ordering, which is the convention adopted by C and Python (numpy) among other languages. The ordering can be verified using the `vec()` function or the syntax `[:]`:
"""

# ╔═╡ 2222cebe-a13c-4989-8cf1-cf42cbfc6252
B = rand(0:9,4,3)

# ╔═╡ 13ebee2e-0798-4d39-a1d3-20136077f142
B[:]

# ╔═╡ 0aa4c3aa-0cfd-4711-85df-f4b9dbb88c24
vec(B)

# ╔═╡ 638db6e4-cd90-45d4-9418-2c16b45fad73
md"""
The ordering of arrays can have significant performance effects when looping over arrays. Loops should be organized such that the subsequent accessed elements are close to each other in physical memory.

The following functions accept a `Vector` and and return a square `Array` with the rows or the columns filled with copies of the input vector, respectively.
"""

# ╔═╡ f7ca72fc-89c5-4764-87d3-9faa7af1ef23
function copy_cols{T}(x::Vector{T})
    n = size(x, 1)
    out = Array{eltype(x)}(n, n)
    for i=1:n
        out[:, i] = x
    end
    out
end

function copy_rows{T}(x::Vector{T})
    n = size(x, 1)
    out = Array{eltype(x)}(n, n)
    for i=1:n
        out[i, :] = x
    end
    out
end

# ╔═╡ cf8dad07-37bd-4530-b482-219ec0991e86
copy_cols([1.0,2])
copy_rows([1.0,2])

# ╔═╡ 7999b00f-65d9-4ee1-9b6b-1c764d18122e
x=rand(5000) # generate a random vector

# ╔═╡ 240d264c-0f2c-42c5-affa-f9a67b2df5e0
@time C=copy_cols(x)  # We generate a large matrix

# ╔═╡ 730e6495-41fb-4a52-b9ec-07b2b1e2f063
@time D=copy_rows(x); # This is few times slower

# ╔═╡ 56d0e42a-fba2-4499-92fb-47253885bd8f
md"""
### Remark
There is also a built-in function `repmat()`:
"""

# ╔═╡ 60cd47ae-f366-4dbe-953b-fcfeb5dd122b
?repmat

# ╔═╡ 1472d737-e4a8-4cd1-8896-b60248435d24
@time C1=repmat(x,1,5000);

# ╔═╡ 85c2927f-0302-4157-a137-e78879fcadb4


# ╔═╡ Cell order:
# ╟─d54b3cd3-c52c-4db9-9267-2e8871cf4597
# ╟─b8c1b263-a5c5-45a2-afdc-744bdc25ca54
# ╠═23d162f9-184b-415a-87a2-ccf75b6d5e9a
# ╟─bb8a280d-b118-4842-9220-9382a5408ed5
# ╠═b000b740-72e4-4056-98d9-c92608cd3a10
# ╠═28ed4062-6ad6-4a60-b772-006869bb504f
# ╠═e27d795b-7d7d-4e76-9ede-9e33b902c2cf
# ╠═299b40a4-4d21-46a4-b7ac-133f69a8309b
# ╟─9042a5c9-97bf-4177-9585-25c4bcbd611f
# ╠═a9691c4d-7f2c-461c-a42d-cfb5209f3a2d
# ╠═11d575a0-7311-4e1f-884d-f447a888602c
# ╟─e5545357-9216-46f4-98d1-dfd4a3f31ff7
# ╠═6a9fd473-7939-42f6-a5e4-53dc53abe15f
# ╠═93e801a3-c255-428f-878c-60c34d0d7870
# ╠═401c72ac-07e7-4ed7-9735-e2d8c846633d
# ╟─1ec9ad17-67b6-4778-abbd-e31b4d2b2887
# ╠═557aeae6-93cb-419b-88d5-274018a95d84
# ╠═8b754531-102c-4d8f-905d-afd44a354aeb
# ╠═a3ca5d12-c330-488b-8639-ad36bc98d3e7
# ╟─6571ae43-6583-47f9-bd6a-abe1d3a0f85a
# ╠═236594df-0138-4bdd-be1e-bd6a4f69fa02
# ╠═692f908f-8b0b-4809-bafe-d64840650a52
# ╟─20fd5340-7aae-4d2b-9da9-27fe15dd1789
# ╠═f12d97f8-498c-4a64-98ec-73486f875728
# ╠═38fad48c-2bc4-433b-80c7-b685f3076691
# ╟─94ce85a1-f6bc-483e-a3b3-70f8e4397e54
# ╠═02bed44e-1265-4394-87ba-e3e2c617067b
# ╠═328f3f81-4894-4b76-8d49-274fde7da898
# ╟─9d7a39c7-acc1-4d40-878d-32a367596d81
# ╟─2cf93c7e-aa9e-4c0f-8ea5-7e5985e38632
# ╠═240e99da-c557-43b8-8ecc-f09542591f93
# ╠═07a499e7-6399-42e1-9677-a00d5bf95970
# ╟─18f0d086-514a-4017-852e-ff172a7476fd
# ╟─6845a906-c76e-46ec-b689-ff7cbadcbe37
# ╠═291af947-08a0-4f61-85a2-f07f7b428833
# ╠═0fb1efd5-6a54-4963-901d-27eec5a1d0a8
# ╠═cb0027b5-7afb-4d00-a5eb-a01e32cb5f7f
# ╟─44a8c386-f5fe-497e-b8b3-829c575d9a11
# ╠═2222cebe-a13c-4989-8cf1-cf42cbfc6252
# ╠═13ebee2e-0798-4d39-a1d3-20136077f142
# ╠═0aa4c3aa-0cfd-4711-85df-f4b9dbb88c24
# ╟─638db6e4-cd90-45d4-9418-2c16b45fad73
# ╠═f7ca72fc-89c5-4764-87d3-9faa7af1ef23
# ╠═cf8dad07-37bd-4530-b482-219ec0991e86
# ╠═7999b00f-65d9-4ee1-9b6b-1c764d18122e
# ╠═240d264c-0f2c-42c5-affa-f9a67b2df5e0
# ╠═730e6495-41fb-4a52-b9ec-07b2b1e2f063
# ╟─56d0e42a-fba2-4499-92fb-47253885bd8f
# ╠═60cd47ae-f366-4dbe-953b-fcfeb5dd122b
# ╠═1472d737-e4a8-4cd1-8896-b60248435d24
# ╠═85c2927f-0302-4157-a137-e78879fcadb4
