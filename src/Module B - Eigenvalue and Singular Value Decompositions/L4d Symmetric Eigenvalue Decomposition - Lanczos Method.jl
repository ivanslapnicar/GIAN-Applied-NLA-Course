### A Pluto.jl notebook ###
# v0.14.2

using Markdown
using InteractiveUtils

# ╔═╡ 1060b410-1c26-4d2a-bd23-baa3c8682890
begin
	using PlutoUI
	PlutoUI.TableOfContents(aside=true)
end

# ╔═╡ 991ff3f5-6131-4ce9-92b3-a6c1175d11f2
using LinearAlgebra

# ╔═╡ 9a5cc700-b014-45c1-a239-d5d02f85c754
begin
	using Random
	Random.seed!(421)
	n=100
	A=Matrix(Symmetric(rand(n,n)))
	# Or: A = rand(5,5) |> t -> t + t'
	x=rand(n)
	k=10
end

# ╔═╡ e7a7eb51-ed15-4015-82c3-061dba368fc3
# ARPACK implementation
using Arpack

# ╔═╡ 45b96d60-1a04-4924-9d7d-324def90d004
using LinearMaps

# ╔═╡ 5865e977-faa9-432b-a014-edb41d1324aa
using SparseArrays

# ╔═╡ 779bf5a8-0463-4f07-b37a-7e1d3e1ab224
md"""
# Symmetric Eigenvalue Decomposition - Lanczos Method


If the matrix $A$ is large and sparse and/or if only some eigenvalues and their eigenvectors are desired, iterative methods are the methods of choice. For example, the power method can be useful to compute the eigenvalue with the largest modulus. The basic operation in the power method is matrix-vector multiplication, and this can be performed very fast if $A$ is sparse. Moreover, $A$ need not be stored in the computer -- the input for the algorithm can be just a function which, given some vector $x$, returns the product $Ax$.

An _improved_ version of the power method, which efficiently computes some eigenvalues (either largest in modulus or near some target value $\mu$) and the corresponding eigenvectors, is the Lanczos method.

For more details, see  [I. Slapničar, Symmetric Matrix Eigenvalue Techniques, pp. 55.1-55.25](https://www.routledge.com/Handbook-of-Linear-Algebra/Hogben/p/book/9781138199897) and the references therein.

__Prerequisites__

The reader should be familiar with concepts of eigenvalues and eigenvectors, related perturbation theory, and algorithms. 

 
__Competences__

The reader should be able to recognise matrices which warrant use uf Lanczos method, to apply the method and to assess the accuracy of the solution.
"""

# ╔═╡ 7098223a-021d-43b1-8dd7-89725daf64aa
md"""
# Lanczos method

 $A$ is a real symmetric matrix of order $n$.

## Definitions

Given a nonzero vector $x$ and an index $k<n$, the __Krylov matrix__ is defined as

$$K_k=\begin{bmatrix} x & Ax & A^2 x &\cdots & A^{k-1}x \end{bmatrix}.$$

__Krylov subspace__ is the subspace spanned by the columns of $K_k$.

## Facts

1. The Lanczos method is based on the following observation. If $K_k=XR$ is the $QR$ factorization of the matrix $K_k$, then the $k\times k$ matrix $T=X^T A X$ is tridiagonal. The matrices $X$ and $T$ can be computed by using only matrix-vector products in $O(kn)$ operations.

2. Let $T=Q\Lambda Q^T$ be the EVD of $T$. Then $\lambda_i$ approximate well some of the largest and smallest eigenvalues of $A$, and the columns of the matrix $U=XQ$ approximate the corresponding eigenvectors.

3. As $k$ increases, the largest (smallest) eigenvalues of the matrix $T_{1:k,1:k}$ converge towards some of the largest (smallest) eigenvalues of $A$ (due to the Cauchy interlace property). The algorithm can be redesigned to compute only largest or smallest eigenvalues. Also, by using shift and invert strategy, the method can be used to compute eigenvalues near some specified value. In order to obtain better approximations, $k$ should be greater than the number of required eigenvalues. On the other side, in order to obtain better accuracy and efficacy, $k$ should be as small as possible.

4. The last computed element, $\mu=T_{k+1,k}$, provides information about accuracy:

$$\begin{aligned}
\|AU-U\Lambda\|_2&=\mu, \\
\|AU_{:,i}-\lambda_i U_{:,i}\|_2&=\mu |Q_{ki}|, \quad  i=1,\ldots,k.
\end{aligned}$$

Further, there are $k$ eigenvalues $\tilde\lambda_1,\ldots,\tilde\lambda_k$ of $A$ such that $|\lambda_i-\tilde\lambda_i|\leq \mu$, and for the corresponding eigenvectors, we have 

$$\sin2\Theta(U_{:,i},\tilde U_{:,i}) \leq \frac{2\mu}{\min_{j\neq i} |\lambda_i-\tilde \lambda_j|}.$$ 

5. In practical implementations, $\mu$ is usually used to determine the index $k$. 

6. The Lanczos method has inherent numerical instability in the floating-point arithmetic: since the Krylov vectors are, in fact, generated by the power method, they converge towards an eigenvector of $A$. Thus, as $k$ increases, the Krylov vectors become more and more parallel, and the recursion in the function `myLanczos()` becomes numerically unstable and the computed columns of $X$ cease to be sufficiently orthogonal. This affects both the convergence and the accuracy of the algorithm. For example, several eigenvalues of $T$ may converge towards a simple eigenvalue of $A$ (the, so called, __ghost eigenvalues__).

7. The loss of orthogonality is dealt with by using the __full reorthogonalization__ procedure: in each step, the new ${\bf z}$ is orthogonalized against all previous columns of $X$, that is, in function `myLanczos()`, the formula 
```
z=z-Tr.dv[i]*X[:,i]-Tr.ev[i-1]*X[:,i-1]
```
is replaced by
```
z=z-sum(dot(z,Tr.dv[i])*X[:,i]-Tr.ev[i-1]*X[:,i-1]
```
To obtain better orthogonality, the latter formula is usually executed twice. The full reorthogonalization raises the operation count to $O(k^2n)$.

8. The __selective re-orthogonalization__ is the procedure in which the current $z$ is orthogonalized against some selected columns of $X$, in order to attain sufficient numerical stability and not increase the operation count too much. The details are very subtle and can be found in the references.
  
9. The Lanczos method is usually used for sparse matrices. Sparse matrix $A$ is stored in the sparse format in which only values and indices of nonzero elements are stored. The number of operations required to multiply some vector by $A$ is also proportional to the number of nonzero elements.
  
10. The function `eigs()` from ther package [Arpack.jl](https://github.com/JuliaLinearAlgebra/Arpack.jl) implements Lanczos method real for symmetric matrices and more general Arnoldi method for general matrices. This package is a wrapper for the well-known [ARPACK Library](https://www.caam.rice.edu/software/ARPACK/).
"""

# ╔═╡ 132112e1-416b-4c38-bc41-8ea04ba5e9c7
md"
## Examples
"

# ╔═╡ 786943c0-50e9-4991-bf1d-def1f004f40f
function Lanczos(A::Array{T}, x::Vector{T}, k::Int) where T
    n=size(A,1)
    X=Array{T}(undef,n,k)
    dv=Array{T}(undef,k)
    ev=Array{T}(undef,k-1)
    X[:,1]=x/norm(x)
    for i=1:k-1
        z=A*X[:,i]
        dv[i]=X[:,i]⋅z
        # Three-term recursion
        if i==1
            z=z-dv[i]*X[:,i]
        else
            # z=z-dv[i]*X[:,i]-ev[i-1]*X[:,i-1]
            # Full reorthogonalization - once or even twice
            z=z-sum([(z⋅X[:,j])*X[:,j] for j=1:i])
            # z=z-sum([(z⋅X[:,j])*X[:,j] for j=1:i])
        end
        μ=norm(z)
        if μ==0
            Tr=SymTridiagonal(dv[1:i-1],ev[1:i-2])
            return eigvals(Tr), X[:,1:i-1]*eigvecs(Tr), X[:,1:i-1], μ
        else
            ev[i]=μ
            X[:,i+1]=z/μ
        end
    end
    # Last step
    z=A*X[:,end]
    dv[end]=X[:,end]⋅z
    z=z-dv[end]*X[:,end]-ev[end]*X[:,end-1]
    μ=norm(z)
    Tr=SymTridiagonal(dv,ev)
    eigvals(Tr), X*eigvecs(Tr), X, μ
end

# ╔═╡ 29f434ab-5bfa-4148-b6a7-3724ea6c0f25
λ,U,X,μ=Lanczos(A,x,k)

# ╔═╡ 470f381a-a971-4ebc-a08b-56e4e804e1bf
# Orthogonality
norm(X'*X-I)

# ╔═╡ f67e816e-6bbc-412e-9db3-f7b6a845adda
# Tridiagonalization
X'*A*X

# ╔═╡ 01378e27-34e9-423a-86ea-fdc340ca55bb
# Residual
norm(A*U-U*Diagonal(λ)), μ

# ╔═╡ 6d92aa87-39fa-4ff8-a344-b98fd3fc47d6
U'*A*U

# ╔═╡ d809159c-5989-4fa6-bc5e-7488f6fa0637
# Orthogonality
norm(U'*U-I)

# ╔═╡ 261c94ef-5f82-42f8-a01a-0e76aa65cb63
# Full eigenvalue decomposition
λeigen,Ueigen=eigen(A);

# ╔═╡ 6b0f2e46-02d8-4495-921d-85ef80a297bc
# ?eigs

# ╔═╡ 6524faf5-fd9d-4a97-a6da-b6c092ee626c
# Lanczos method implemented in Julia
λeigs,Ueigs=eigs(A; nev=k, which=:LM, ritzvec=true, v0=x)

# ╔═╡ 3bc1dc64-2d58-4296-a661-d2c83a1da5e7
[λ λeigs λeigen[sortperm(abs.(λeigen),rev=true)[1:k]] ]

# ╔═╡ e2bb2486-ab84-4f1c-9f74-f664a6878691
md"""
We see that `eigs()` computes `k` eigenvalues with largest modulus. What eigenvalues did `Lanczos()` compute?
"""

# ╔═╡ 31037f93-2714-4ed3-bc9c-fb290a1f3d7b
for i=1:k
    println(minimum(abs,λeigen.-λ[i]))
end

# ╔═╡ 7f98a584-922a-40c3-bed0-24f335fb7c19
md"""
Conslusion is that the naive implementation of Lanczos is not enough. However, it is fine, when all eigenvalues are computed. Why?
"""

# ╔═╡ a9bc9727-555f-4bdc-bde6-303680b6961b
λall,Uall,Xall,μall=Lanczos(A,x,100)

# ╔═╡ 10d41c8b-772e-452b-a204-7e0b9bf0cd2e
# Residual and relative errors 
norm(A*Uall-Uall*Diagonal(λall)), norm((λeigen-λall)./λeigen)

# ╔═╡ 062dcd0f-f521-4b65-a8c6-7caf193bfe8e
methods(eigs);

# ╔═╡ c1b25610-a6d3-4ac9-b4e6-092d8be727d4
md"""
# Operator version

We can use Lanczos method with operator which, given vector `x`, returns the product `A*x`. We use the function `LinearMap()` from the package [LinearMaps.jl](https://github.com/Jutho/LinearMaps.jl).
"""

# ╔═╡ c38ad9d9-ec39-4408-b894-6c5eb2c47484
methods(LinearMap);

# ╔═╡ f6b13884-5f9a-4b36-9796-4939a0b596a1
# Operator from the matrix
C=LinearMap(A)

# ╔═╡ 9085c992-4db8-4233-923d-cd9ed2dd38d7
begin
	λC,UC=eigs(C; nev=k, which=:LM, ritzvec=true, v0=x)
	norm(λeigs-λC)
end

# ╔═╡ d50142d7-e969-44f2-b3b1-77274d3f1dd7
md"""
Here is an example of `LinearMap()` with the function. 
"""

# ╔═╡ 439133a4-aef2-444d-a9d6-231439093ea2
f(x)=A*x

# ╔═╡ 13de141f-9ee8-49d5-ad95-ff03ebcb5ae6
D=LinearMap(f,n,issymmetric=true)

# ╔═╡ 22c9fb03-49e3-48af-a388-fea238b8a055
begin
	λD,UD=eigs(D, nev=k, which=:LM, ritzvec=true, v0=x)
	norm(λeigs-λD)
end

# ╔═╡ 12455f68-8a1a-48f4-b218-b29c075545bf
md"""
# Sparse matrices
"""

# ╔═╡ 86d683c6-ed43-4b7b-8aee-91c05907bb77
# ?sprand

# ╔═╡ a02ab76c-b8d2-4445-94df-10bb2a4b7467
# Generate a sparse symmetric matrix
E=sprand(n,n,0.05) |> t -> t+t'

# ╔═╡ a3281500-5679-4a90-b205-2a2876d9e1bd
issymmetric(E)

# ╔═╡ edb02611-09a0-414f-b744-8c33ae85885b
eigs(E; nev=k, which=:LM, ritzvec=true, v0=x)

# ╔═╡ Cell order:
# ╟─1060b410-1c26-4d2a-bd23-baa3c8682890
# ╟─779bf5a8-0463-4f07-b37a-7e1d3e1ab224
# ╟─7098223a-021d-43b1-8dd7-89725daf64aa
# ╟─132112e1-416b-4c38-bc41-8ea04ba5e9c7
# ╠═991ff3f5-6131-4ce9-92b3-a6c1175d11f2
# ╠═786943c0-50e9-4991-bf1d-def1f004f40f
# ╠═9a5cc700-b014-45c1-a239-d5d02f85c754
# ╠═29f434ab-5bfa-4148-b6a7-3724ea6c0f25
# ╠═470f381a-a971-4ebc-a08b-56e4e804e1bf
# ╠═f67e816e-6bbc-412e-9db3-f7b6a845adda
# ╠═01378e27-34e9-423a-86ea-fdc340ca55bb
# ╠═6d92aa87-39fa-4ff8-a344-b98fd3fc47d6
# ╠═d809159c-5989-4fa6-bc5e-7488f6fa0637
# ╠═261c94ef-5f82-42f8-a01a-0e76aa65cb63
# ╠═e7a7eb51-ed15-4015-82c3-061dba368fc3
# ╠═6b0f2e46-02d8-4495-921d-85ef80a297bc
# ╠═6524faf5-fd9d-4a97-a6da-b6c092ee626c
# ╠═3bc1dc64-2d58-4296-a661-d2c83a1da5e7
# ╟─e2bb2486-ab84-4f1c-9f74-f664a6878691
# ╠═31037f93-2714-4ed3-bc9c-fb290a1f3d7b
# ╟─7f98a584-922a-40c3-bed0-24f335fb7c19
# ╠═a9bc9727-555f-4bdc-bde6-303680b6961b
# ╠═10d41c8b-772e-452b-a204-7e0b9bf0cd2e
# ╠═062dcd0f-f521-4b65-a8c6-7caf193bfe8e
# ╟─c1b25610-a6d3-4ac9-b4e6-092d8be727d4
# ╠═45b96d60-1a04-4924-9d7d-324def90d004
# ╠═c38ad9d9-ec39-4408-b894-6c5eb2c47484
# ╠═f6b13884-5f9a-4b36-9796-4939a0b596a1
# ╠═9085c992-4db8-4233-923d-cd9ed2dd38d7
# ╟─d50142d7-e969-44f2-b3b1-77274d3f1dd7
# ╠═439133a4-aef2-444d-a9d6-231439093ea2
# ╠═13de141f-9ee8-49d5-ad95-ff03ebcb5ae6
# ╠═22c9fb03-49e3-48af-a388-fea238b8a055
# ╟─12455f68-8a1a-48f4-b218-b29c075545bf
# ╠═5865e977-faa9-432b-a014-edb41d1324aa
# ╠═86d683c6-ed43-4b7b-8aee-91c05907bb77
# ╠═a02ab76c-b8d2-4445-94df-10bb2a4b7467
# ╠═a3281500-5679-4a90-b205-2a2876d9e1bd
# ╠═edb02611-09a0-414f-b744-8c33ae85885b
